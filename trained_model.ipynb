{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dragonfire-09/projects/blob/main/trained_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUprnKtfKFfW",
        "outputId": "30f1b9fd-ff69-46ff-8841-bcd5b36ec232"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Starting data loading process...\n",
            "Found 1 columns:\n",
            "['Programme;Project Number;CORDIS Link;Project Start Year;Project End Date;Project End Year;Project Title;Project Acronym;Project Status;Call ID;Call Deadline Date;Call Deadline Year;Project Signature Date;Project Signature Year;Project Start Date;Thematic Priority Descr;Pillar Abbr;Pillar Descr;Topic Code;Topic Descr;Simplified ToA;Legal Name;General PIC;Partner Role;Partner Type;Legal Entity Type;Signed Grants;Participation;EU Contribution;Total Cost']\n",
            "Warning: Line 4315 has 3 fields (expected 1)\n",
            "Warning: Line 4316 has 3 fields (expected 1)\n",
            "Warning: Line 4317 has 3 fields (expected 1)\n",
            "Warning: Line 4318 has 3 fields (expected 1)\n",
            "Warning: Line 4319 has 3 fields (expected 1)\n",
            "Warning: Line 4320 has 3 fields (expected 1)\n",
            "Warning: Line 4321 has 3 fields (expected 1)\n",
            "Warning: Line 4322 has 3 fields (expected 1)\n",
            "Warning: Line 4323 has 3 fields (expected 1)\n",
            "Warning: Line 4324 has 3 fields (expected 1)\n",
            "Warning: Line 4325 has 3 fields (expected 1)\n",
            "Warning: Line 4326 has 3 fields (expected 1)\n",
            "Warning: Line 6330 has 3 fields (expected 1)\n",
            "Warning: Line 6331 has 3 fields (expected 1)\n",
            "Warning: Line 6332 has 3 fields (expected 1)\n",
            "Warning: Line 6333 has 3 fields (expected 1)\n",
            "Warning: Line 6334 has 3 fields (expected 1)\n",
            "Warning: Line 6335 has 3 fields (expected 1)\n",
            "Warning: Line 6336 has 3 fields (expected 1)\n",
            "Warning: Line 6337 has 3 fields (expected 1)\n",
            "Warning: Line 6338 has 3 fields (expected 1)\n",
            "Warning: Line 6339 has 3 fields (expected 1)\n",
            "Warning: Line 6340 has 3 fields (expected 1)\n",
            "Warning: Line 6374 has 3 fields (expected 1)\n",
            "Warning: Line 6375 has 3 fields (expected 1)\n",
            "Warning: Line 6376 has 3 fields (expected 1)\n",
            "Warning: Line 6377 has 3 fields (expected 1)\n",
            "Warning: Line 6378 has 3 fields (expected 1)\n",
            "Warning: Line 6379 has 3 fields (expected 1)\n",
            "Warning: Line 6380 has 3 fields (expected 1)\n",
            "Warning: Line 6448 has 3 fields (expected 1)\n",
            "Warning: Line 6449 has 3 fields (expected 1)\n",
            "Warning: Line 6450 has 3 fields (expected 1)\n",
            "Warning: Line 6451 has 3 fields (expected 1)\n",
            "Warning: Line 6452 has 3 fields (expected 1)\n",
            "Warning: Line 6453 has 3 fields (expected 1)\n",
            "Warning: Line 6454 has 3 fields (expected 1)\n",
            "Warning: Line 6455 has 3 fields (expected 1)\n",
            "Warning: Line 6456 has 3 fields (expected 1)\n",
            "Warning: Line 6457 has 3 fields (expected 1)\n",
            "Warning: Line 6458 has 3 fields (expected 1)\n",
            "Warning: Line 6459 has 3 fields (expected 1)\n",
            "Warning: Line 6460 has 3 fields (expected 1)\n",
            "Warning: Line 6461 has 3 fields (expected 1)\n",
            "Warning: Line 6462 has 3 fields (expected 1)\n",
            "Warning: Line 6463 has 3 fields (expected 1)\n",
            "Warning: Line 6464 has 3 fields (expected 1)\n",
            "Warning: Line 6465 has 3 fields (expected 1)\n",
            "Warning: Line 6466 has 3 fields (expected 1)\n",
            "Warning: Line 6467 has 3 fields (expected 1)\n",
            "Warning: Line 6473 has 3 fields (expected 1)\n",
            "Warning: Line 6474 has 3 fields (expected 1)\n",
            "Warning: Line 6475 has 3 fields (expected 1)\n",
            "Warning: Line 6476 has 3 fields (expected 1)\n",
            "Warning: Line 6477 has 3 fields (expected 1)\n",
            "Warning: Line 6478 has 3 fields (expected 1)\n",
            "Warning: Line 6479 has 3 fields (expected 1)\n",
            "Warning: Line 6480 has 3 fields (expected 1)\n",
            "Warning: Line 6481 has 3 fields (expected 1)\n",
            "Warning: Line 6482 has 3 fields (expected 1)\n",
            "Warning: Line 6483 has 3 fields (expected 1)\n",
            "Warning: Line 6484 has 3 fields (expected 1)\n",
            "Warning: Line 6485 has 3 fields (expected 1)\n",
            "Warning: Line 6486 has 3 fields (expected 1)\n",
            "Warning: Line 6487 has 3 fields (expected 1)\n",
            "Warning: Line 6488 has 3 fields (expected 1)\n",
            "Warning: Line 6489 has 3 fields (expected 1)\n",
            "Warning: Line 6490 has 3 fields (expected 1)\n",
            "Warning: Line 6491 has 3 fields (expected 1)\n",
            "Warning: Line 6492 has 3 fields (expected 1)\n",
            "Warning: Line 6493 has 3 fields (expected 1)\n",
            "Warning: Line 6494 has 3 fields (expected 1)\n",
            "Warning: Line 6495 has 3 fields (expected 1)\n",
            "Warning: Line 6496 has 3 fields (expected 1)\n",
            "Processed 10000 lines...\n",
            "Processed 20000 lines...\n",
            "Processed 30000 lines...\n",
            "Processed 40000 lines...\n",
            "Processed 50000 lines...\n",
            "Processed 60000 lines...\n",
            "Creating DataFrame...\n",
            "\n",
            "Processing data...\n",
            "Starting data cleaning and processing...\n",
            "Performing basic cleaning...\n",
            "Processing specific columns...\n",
            "Creating derived features...\n",
            "\n",
            "Analyzing data...\n",
            "\n",
            "Data Analysis:\n",
            "\n",
            "Dataset Shape: (63985, 1)\n",
            "\n",
            "Columns:\n",
            "Programme;Project Number;CORDIS Link;Project Start Year;Project End Date;Project End Year;Project Title;Project Acronym;Project Status;Call ID;Call Deadline Date;Call Deadline Year;Project Signature Date;Project Signature Year;Project Start Date;Thematic Priority Descr;Pillar Abbr;Pillar Descr;Topic Code;Topic Descr;Simplified ToA;Legal Name;General PIC;Partner Role;Partner Type;Legal Entity Type;Signed Grants;Participation;EU Contribution;Total Cost: 63828 non-null values, dtype: object\n",
            "\n",
            "Sample of numeric columns:\n",
            "\n",
            "Saving processed data...\n",
            "Data saved to 'processed_horizon_data.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import sys\n",
        "from io import StringIO\n",
        "\n",
        "# Increase CSV field size limit\n",
        "maxInt = sys.maxsize\n",
        "while True:\n",
        "    try:\n",
        "        csv.field_size_limit(maxInt)\n",
        "        break\n",
        "    except OverflowError:\n",
        "        maxInt = int(maxInt/10)\n",
        "\n",
        "def read_large_csv(file_path):\n",
        "    \"\"\"\n",
        "    Read large CSV files with custom parsing\n",
        "    \"\"\"\n",
        "    print(\"Starting data loading process...\")\n",
        "\n",
        "    # Initialize empty lists for data\n",
        "    rows = []\n",
        "    headers = None\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
        "            # Read first line to get headers\n",
        "            headers = file.readline().strip().split('\\t')\n",
        "            print(f\"Found {len(headers)} columns:\")\n",
        "            print(headers)\n",
        "\n",
        "            # Process rest of the file\n",
        "            for line_num, line in enumerate(file, 2):\n",
        "                try:\n",
        "                    # Split line by tab\n",
        "                    fields = line.strip().split('\\t')\n",
        "\n",
        "                    # Handle inconsistent number of fields\n",
        "                    if len(fields) != len(headers):\n",
        "                        print(f\"Warning: Line {line_num} has {len(fields)} fields (expected {len(headers)})\")\n",
        "                        # Adjust field count to match headers\n",
        "                        if len(fields) > len(headers):\n",
        "                            fields = fields[:len(headers)]\n",
        "                        else:\n",
        "                            fields.extend([''] * (len(headers) - len(fields)))\n",
        "\n",
        "                    rows.append(fields)\n",
        "\n",
        "                    # Print progress\n",
        "                    if line_num % 10000 == 0:\n",
        "                        print(f\"Processed {line_num} lines...\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing line {line_num}: {e}\")\n",
        "                    continue\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "        return None\n",
        "\n",
        "    print(\"Creating DataFrame...\")\n",
        "    df = pd.DataFrame(rows, columns=headers)\n",
        "    return df\n",
        "\n",
        "def clean_and_process_data(df):\n",
        "    \"\"\"\n",
        "    Clean and process the loaded data\n",
        "    \"\"\"\n",
        "    print(\"Starting data cleaning and processing...\")\n",
        "\n",
        "    # 1. Basic cleaning\n",
        "    print(\"Performing basic cleaning...\")\n",
        "    df = df.replace('', np.nan)\n",
        "    df = df.replace('None', np.nan)\n",
        "\n",
        "    # 2. Clean numeric columns\n",
        "    def clean_numeric(x):\n",
        "        if pd.isna(x):\n",
        "            return 0\n",
        "        try:\n",
        "            # Remove any non-numeric characters except decimal point and minus\n",
        "            cleaned = ''.join(char for char in str(x) if char.isdigit() or char in '.-')\n",
        "            return float(cleaned) if cleaned else 0\n",
        "        except:\n",
        "            return 0\n",
        "\n",
        "    # 3. Process specific columns\n",
        "    print(\"Processing specific columns...\")\n",
        "\n",
        "    # Financial columns\n",
        "    numeric_columns = ['EU Contribution', 'Total Cost']\n",
        "    for col in numeric_columns:\n",
        "        if col in df.columns:\n",
        "            df[f'{col}_Clean'] = df[col].apply(clean_numeric)\n",
        "            print(f\"Processed {col}\")\n",
        "\n",
        "    # Dates\n",
        "    date_columns = ['Project Start Date', 'Project End Date', 'Call Deadline Date']\n",
        "    for col in date_columns:\n",
        "        if col in df.columns:\n",
        "            df[f'{col}_Clean'] = pd.to_datetime(df[col], errors='coerce')\n",
        "            print(f\"Processed {col}\")\n",
        "\n",
        "    # 4. Create derived features\n",
        "    print(\"Creating derived features...\")\n",
        "    if 'Project Start Date_Clean' in df.columns and 'Project End Date_Clean' in df.columns:\n",
        "        df['Project_Duration_Days'] = (df['Project End Date_Clean'] - df['Project Start Date_Clean']).dt.days\n",
        "\n",
        "    return df\n",
        "\n",
        "def analyze_data(df):\n",
        "    \"\"\"\n",
        "    Perform basic data analysis\n",
        "    \"\"\"\n",
        "    print(\"\\nData Analysis:\")\n",
        "    print(\"\\nDataset Shape:\", df.shape)\n",
        "\n",
        "    print(\"\\nColumns:\")\n",
        "    for col in df.columns:\n",
        "        non_null = df[col].count()\n",
        "        dtype = df[col].dtype\n",
        "        print(f\"{col}: {non_null} non-null values, dtype: {dtype}\")\n",
        "\n",
        "    print(\"\\nSample of numeric columns:\")\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    if len(numeric_cols) > 0:\n",
        "        print(df[numeric_cols].describe())\n",
        "\n",
        "def main():\n",
        "    # File path\n",
        "    file_path = '/content/Horizon projects dataset .csv'\n",
        "\n",
        "    # Load data\n",
        "    print(\"Loading data...\")\n",
        "    df = read_large_csv(file_path)\n",
        "\n",
        "    if df is not None:\n",
        "        # Process data\n",
        "        print(\"\\nProcessing data...\")\n",
        "        df_processed = clean_and_process_data(df)\n",
        "\n",
        "        # Analyze data\n",
        "        print(\"\\nAnalyzing data...\")\n",
        "        analyze_data(df_processed)\n",
        "\n",
        "        # Save processed data\n",
        "        print(\"\\nSaving processed data...\")\n",
        "        df_processed.to_csv('processed_horizon_data.csv', index=False)\n",
        "        print(\"Data saved to 'processed_horizon_data.csv'\")\n",
        "\n",
        "        return df_processed\n",
        "    else:\n",
        "        print(\"Failed to load data\")\n",
        "        return None\n",
        "\n",
        "# Run the script\n",
        "if __name__ == \"__main__\":\n",
        "    processed_df = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvpCrcLjQcVh",
        "outputId": "7d528b7f-bb7a-47d8-af20-1ec72e93864a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in the dataset:\n",
            "['Programme;Project Number;CORDIS Link;Project Start Year;Project End Date;Project End Year;Project Title;Project Acronym;Project Status;Call ID;Call Deadline Date;Call Deadline Year;Project Signature Date;Project Signature Year;Project Start Date;Thematic Priority Descr;Pillar Abbr;Pillar Descr;Topic Code;Topic Descr;Simplified ToA;Legal Name;General PIC;Partner Role;Partner Type;Legal Entity Type;Signed Grants;Participation;EU Contribution;Total Cost']\n",
            "\n",
            "First few rows:\n",
            "  Programme;Project Number;CORDIS Link;Project Start Year;Project End Date;Project End Year;Project Title;Project Acronym;Project Status;Call ID;Call Deadline Date;Call Deadline Year;Project Signature Date;Project Signature Year;Project Start Date;Thematic Priority Descr;Pillar Abbr;Pillar Descr;Topic Code;Topic Descr;Simplified ToA;Legal Name;General PIC;Partner Role;Partner Type;Legal Entity Type;Signed Grants;Participation;EU Contribution;Total Cost\n",
            "0  H2020;115797;http://cordis.europa.eu/project/i...                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
            "1  H2020;115797;http://cordis.europa.eu/project/i...                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
            "2  H2020;115797;http://cordis.europa.eu/project/i...                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
            "3  H2020;115797;http://cordis.europa.eu/project/i...                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
            "4  H2020;115797;http://cordis.europa.eu/project/i...                                                                                                                                                                                                                                                                                                                                                                                                                    \n"
          ]
        }
      ],
      "source": [
        "# First, let's look at the data structure\n",
        "import pandas as pd\n",
        "\n",
        "# Load and examine the data\n",
        "df = pd.read_csv('processed_horizon_data.csv')\n",
        "print(\"Columns in the dataset:\")\n",
        "print(df.columns.tolist())\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "class FastAcademicAdvisor:\n",
        "    def __init__(self, data_path='processed_horizon_data.csv'):\n",
        "        self.data_path = data_path\n",
        "        self.encoder = SentenceTransformer('paraphrase-MiniLM-L3-v2')\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.embeddings = None\n",
        "        self.knowledge_base = {}\n",
        "\n",
        "        self.categories = [\n",
        "            'eligibility',\n",
        "            'funding',\n",
        "            'application',\n",
        "            'topic',\n",
        "            'partners'\n",
        "        ]\n",
        "\n",
        "    def process_data(self):\n",
        "        \"\"\"\n",
        "        Process data with error handling\n",
        "        \"\"\"\n",
        "        print(\"Processing data...\")\n",
        "\n",
        "        # Read data with proper handling\n",
        "        try:\n",
        "            df = pd.read_csv(\n",
        "                self.data_path,\n",
        "                sep=';',\n",
        "                low_memory=False  # Handle mixed types\n",
        "            )\n",
        "\n",
        "            # Clean and prepare text data\n",
        "            print(\"Cleaning data...\")\n",
        "            df = self.clean_dataframe(df)\n",
        "\n",
        "            # Create embeddings\n",
        "            print(\"Creating embeddings...\")\n",
        "            texts = df['combined_text'].tolist()\n",
        "            texts = [str(text) for text in texts]  # Ensure all texts are strings\n",
        "\n",
        "            # Process in batches\n",
        "            batch_size = 32\n",
        "            embeddings = []\n",
        "\n",
        "            for i in tqdm(range(0, len(texts), batch_size)):\n",
        "                batch = texts[i:i + batch_size]\n",
        "                try:\n",
        "                    batch_embeddings = self.encoder.encode(batch)\n",
        "                    embeddings.extend(batch_embeddings)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing batch {i}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            self.embeddings = np.array(embeddings)\n",
        "\n",
        "            # Build knowledge base\n",
        "            print(\"Building knowledge base...\")\n",
        "            for idx, row in df.iterrows():\n",
        "                self.knowledge_base[idx] = {\n",
        "                    'title': str(row['Project Title']),\n",
        "                    'topic': str(row['Topic Descr']),\n",
        "                    'funding': self.clean_numeric(row['EU Contribution']),\n",
        "                    'type': str(row['Legal Entity Type']),\n",
        "                    'status': str(row['Project Status'])\n",
        "                }\n",
        "\n",
        "            print(\"Processing complete!\")\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing data: {e}\")\n",
        "            return None\n",
        "\n",
        "    def clean_dataframe(self, df):\n",
        "        \"\"\"\n",
        "        Clean and prepare DataFrame\n",
        "        \"\"\"\n",
        "        # Handle missing values\n",
        "        df = df.fillna('')\n",
        "\n",
        "        # Clean text columns\n",
        "        text_columns = ['Project Title', 'Topic Descr', 'Thematic Priority Descr']\n",
        "        for col in text_columns:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].astype(str).apply(self.clean_text)\n",
        "\n",
        "        # Combine text for embeddings\n",
        "        df['combined_text'] = df.apply(\n",
        "            lambda x: f\"{x['Project Title']} {x['Topic Descr']} {x['Thematic Priority Descr']}\",\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def clean_text(text):\n",
        "        \"\"\"\n",
        "        Clean text data\n",
        "        \"\"\"\n",
        "        # Convert to string and clean\n",
        "        text = str(text)\n",
        "        text = text.lower()\n",
        "        text = ' '.join(text.split())\n",
        "        return text\n",
        "\n",
        "    @staticmethod\n",
        "    def clean_numeric(value):\n",
        "        \"\"\"\n",
        "        Clean numeric values\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if pd.isna(value):\n",
        "                return 0\n",
        "            value = str(value).replace(',', '').replace('€', '').strip()\n",
        "            return float(value) if value else 0\n",
        "        except:\n",
        "            return 0\n",
        "\n",
        "    def save_model(self, filename='fast_academic_advisor.pkl'):\n",
        "        \"\"\"\n",
        "        Save model data\n",
        "        \"\"\"\n",
        "        if self.embeddings is not None:\n",
        "            model_data = {\n",
        "                'embeddings': self.embeddings,\n",
        "                'knowledge_base': self.knowledge_base,\n",
        "                'categories': self.categories\n",
        "            }\n",
        "\n",
        "            with open(filename, 'wb') as f:\n",
        "                pickle.dump(model_data, f)\n",
        "            print(\"Model saved!\")\n",
        "        else:\n",
        "            print(\"No model data to save!\")\n",
        "\n",
        "    def load_model(self, filename='fast_academic_advisor.pkl'):\n",
        "        \"\"\"\n",
        "        Load model data\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with open(filename, 'rb') as f:\n",
        "                model_data = pickle.load(f)\n",
        "\n",
        "            self.embeddings = model_data['embeddings']\n",
        "            self.knowledge_base = model_data['knowledge_base']\n",
        "            self.categories = model_data['categories']\n",
        "            print(\"Model loaded!\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "\n",
        "    def get_response(self, query):\n",
        "        \"\"\"\n",
        "        Generate response for query\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Clean and encode query\n",
        "            query = self.clean_text(query)\n",
        "            query_embedding = self.encoder.encode([query])\n",
        "\n",
        "            # Find similar projects\n",
        "            similarities = np.dot(self.embeddings, query_embedding.T).flatten()\n",
        "            top_indices = similarities.argsort()[-3:][::-1]\n",
        "\n",
        "            # Generate response\n",
        "            response = \"Relevant Projects:\\n\\n\"\n",
        "            for idx in top_indices:\n",
        "                project = self.knowledge_base[idx]\n",
        "                response += f\"Project: {project['title']}\\n\"\n",
        "                response += f\"Topic: {project['topic']}\\n\"\n",
        "                response += f\"Funding: €{project['funding']:,.2f}\\n\"\n",
        "                response += f\"Type: {project['type']}\\n\"\n",
        "                response += f\"Status: {project['status']}\\n\\n\"\n",
        "\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error generating response: {e}\"\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # Initialize\n",
        "        print(\"Initializing Fast Academic Advisor...\")\n",
        "        advisor = FastAcademicAdvisor('processed_horizon_data.csv')\n",
        "\n",
        "        # Process data\n",
        "        df = advisor.process_data()\n",
        "\n",
        "        if df is not None:\n",
        "            # Save model\n",
        "            advisor.save_model()\n",
        "\n",
        "            # Test queries\n",
        "            print(\"\\nTesting queries...\")\n",
        "            test_queries = [\n",
        "                \"AI research projects\",\n",
        "                \"climate change funding\",\n",
        "                \"healthcare innovation\"\n",
        "            ]\n",
        "\n",
        "            for query in test_queries:\n",
        "                print(f\"\\nQuery: {query}\")\n",
        "                response = advisor.get_response(query)\n",
        "                print(response)\n",
        "        else:\n",
        "            print(\"Failed to process data!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main execution: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6hr2YeSPoHh",
        "outputId": "1f434430-5414-40d3-bbb6-660f7fd95d10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Fast Academic Advisor...\n",
            "Processing data...\n",
            "Cleaning data...\n",
            "Creating embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22038/22038 [1:16:41<00:00,  4.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building knowledge base...\n",
            "Processing complete!\n",
            "Model saved!\n",
            "\n",
            "Testing queries...\n",
            "\n",
            "Query: AI research projects\n",
            "Relevant Projects:\n",
            "\n",
            "Project: human-ai teaming platform for maintaining and evolving ai systems in manufacturing\n",
            "Topic: artificial intelligence for manufacturing\n",
            "Funding: €444.92\n",
            "Type: HES\n",
            "Status: CLOSED\n",
            "\n",
            "Project: human-ai teaming platform for maintaining and evolving ai systems in manufacturing\n",
            "Topic: artificial intelligence for manufacturing\n",
            "Funding: €300.00\n",
            "Type: PRC\n",
            "Status: CLOSED\n",
            "\n",
            "Project: human-ai teaming platform for maintaining and evolving ai systems in manufacturing\n",
            "Topic: artificial intelligence for manufacturing\n",
            "Funding: €311.38\n",
            "Type: PRC\n",
            "Status: CLOSED\n",
            "\n",
            "\n",
            "\n",
            "Query: climate change funding\n",
            "Relevant Projects:\n",
            "\n",
            "Project: climate change experiment\n",
            "Topic: -\n",
            "Funding: €0.00\n",
            "Type: -\n",
            "Status: CLOSED\n",
            "\n",
            "Project: climate change experiment\n",
            "Topic: -\n",
            "Funding: €0.00\n",
            "Type: -\n",
            "Status: CLOSED\n",
            "\n",
            "Project: climate change experiment\n",
            "Topic: -\n",
            "Funding: €0.00\n",
            "Type: -\n",
            "Status: CLOSED\n",
            "\n",
            "\n",
            "\n",
            "Query: healthcare innovation\n",
            "Relevant Projects:\n",
            "\n",
            "Project: healthcare innovation procurement network\n",
            "Topic: health care innovation procurement network\n",
            "Funding: €352.73\n",
            "Type: OTH\n",
            "Status: SIGNED\n",
            "\n",
            "Project: healthcare innovation procurement network\n",
            "Topic: health care innovation procurement network\n",
            "Funding: €168.00\n",
            "Type: PUB\n",
            "Status: SIGNED\n",
            "\n",
            "Project: healthcare innovation procurement network\n",
            "Topic: health care innovation procurement network\n",
            "Funding: €400.43\n",
            "Type: PRC\n",
            "Status: SIGNED\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pnBbCfVv5ho0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers datasets scikit-learn nltk pandas numpy torch sentence-transformers\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "import re\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AutoModelForSeq2SeqLM\n",
        "from transformers import TrainingArguments, Trainer, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from datasets import Dataset\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define a function to load and preprocess your data\n",
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Load research funding dataset.\n",
        "    Expected format: JSON or CSV with at least 'question' and 'answer' fields.\n",
        "    Additional metadata like 'category', 'source', etc. are helpful.\n",
        "    \"\"\"\n",
        "    if file_path.endswith('.json'):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "    elif file_path.endswith('.csv'):\n",
        "        data = pd.read_csv(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "    return data\n",
        "\n",
        "# Load your dataset\n",
        "data_path = '/content/processed_horizon_data.csv'  # Update with your actual path\n",
        "try:\n",
        "    data = load_data(data_path)\n",
        "    print(f\"Successfully loaded data with {len(data)} entries\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgfiy0mfiGDf",
        "outputId": "2060dee3-1488-4bee-c616-8e2036d37c77"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Successfully loaded data with 63985 entries\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Input data type: {type(data)}\")\n",
        "if isinstance(data, pd.DataFrame):\n",
        "    print(f\"Input DataFrame shape: {data.shape}\")\n",
        "    print(f\"Input DataFrame columns: {data.columns.tolist()}\")\n",
        "elif isinstance(data, list):\n",
        "    print(f\"Input list length: {len(data)}\")\n",
        "    if len(data) > 0:\n",
        "        print(f\"First item in list: {data[0]}\")\n",
        "else:\n",
        "    print(f\"Unexpected input data type: {type(data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH3ylAwwlMfY",
        "outputId": "dbac25cd-7564-49bf-dc4b-e71a7a2110e1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data type: <class 'pandas.core.frame.DataFrame'>\n",
            "Input DataFrame shape: (63985, 1)\n",
            "Input DataFrame columns: ['Programme;Project Number;CORDIS Link;Project Start Year;Project End Date;Project End Year;Project Title;Project Acronym;Project Status;Call ID;Call Deadline Date;Call Deadline Year;Project Signature Date;Project Signature Year;Project Start Date;Thematic Priority Descr;Pillar Abbr;Pillar Descr;Topic Code;Topic Descr;Simplified ToA;Legal Name;General PIC;Partner Role;Partner Type;Legal Entity Type;Signed Grants;Participation;EU Contribution;Total Cost']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data):\n",
        "    \"\"\"\n",
        "    Preprocess the dataset for model training.\n",
        "    \"\"\"\n",
        "    processed_data = []\n",
        "    skipped_items = 0\n",
        "\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        # Process DataFrame\n",
        "        for index, row in data.iterrows():\n",
        "            if 'question' in row and 'answer' in row:\n",
        "                processed_data.append({\n",
        "                    'question': clean_text(row['question']),\n",
        "                    'answer': clean_text(row['answer']),\n",
        "                    'category': row.get('category', 'general'),\n",
        "                    'index': index  # Add index for debugging\n",
        "                })\n",
        "            else:\n",
        "                skipped_items += 1\n",
        "    else:\n",
        "        # Process JSON list\n",
        "        for index, item in enumerate(data):\n",
        "            if 'question' in item and 'answer' in item:\n",
        "                processed_data.append({\n",
        "                    'question': clean_text(item['question']),\n",
        "                    'answer': clean_text(item['answer']),\n",
        "                    'category': item.get('category', 'general'),\n",
        "                    'index': index  # Add index for debugging\n",
        "                })\n",
        "            else:\n",
        "                skipped_items += 1\n",
        "\n",
        "    print(f\"Processed {len(processed_data)} QA pairs\")\n",
        "    print(f\"Skipped {skipped_items} items due to missing question or answer\")\n",
        "\n",
        "    return processed_data"
      ],
      "metadata": {
        "id": "xInWoGXNlOtk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_data = preprocess_data(data)\n",
        "print(f\"Processed {len(processed_data)} QA pairs\")\n",
        "\n",
        "if len(processed_data) > 0:\n",
        "    print(\"First processed item:\")\n",
        "    print(processed_data[0])\n",
        "else:\n",
        "    print(\"No items were processed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuEuJqSUlaV2",
        "outputId": "da4828ef-37f5-4143-90ce-6bfecee3f123"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 0 QA pairs\n",
            "Skipped 63985 items due to missing question or answer\n",
            "Processed 0 QA pairs\n",
            "No items were processed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load the CSV file with semicolon separator\n",
        "df = pd.read_csv('/content/processed_horizon_data.csv', sep=';')\n",
        "print(\"Original DataFrame shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "# Data preprocessing function\n",
        "def preprocess_horizon_data(df):\n",
        "    \"\"\"\n",
        "    Preprocess the Horizon dataset for model training.\n",
        "    \"\"\"\n",
        "    processed_data = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        qa_pairs = [\n",
        "            {\n",
        "                'question': 'What is the project title and its acronym?',\n",
        "                'answer': f\"Title: {row['Project Title']}\\nAcronym: {row['Project Acronym']}\",\n",
        "                'category': 'project_overview'\n",
        "            },\n",
        "            {\n",
        "                'question': 'What are the project details including timeline and status?',\n",
        "                'answer': f\"Start Date: {row['Project Start Date']}\\nEnd Date: {row['Project End Date']}\\nStatus: {row['Project Status']}\\nThematic Priority: {row['Thematic Priority Descr']}\",\n",
        "                'category': 'project_timeline'\n",
        "            },\n",
        "            {\n",
        "                'question': 'What is the project funding information?',\n",
        "                'answer': f\"EU Contribution: {row['EU Contribution']}\\nTotal Cost: {row['Total Cost']}\",\n",
        "                'category': 'funding'\n",
        "            },\n",
        "            {\n",
        "                'question': 'What are the project topic and pillar details?',\n",
        "                'answer': f\"Pillar: {row['Pillar Descr']}\\nTopic: {row['Topic Descr']}\",\n",
        "                'category': 'classification'\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        processed_data.extend(qa_pairs)\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and normalize text.\"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    #<span class=\"ml-2\" /><span class=\"inline-block w-3 h-3 rounded-full bg-neutral-a12 align-middle mb-[0.1rem]\" />"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJ02rjn8mMWD",
        "outputId": "272972bf-0221-4678-90ed-2509b35b2ac8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame shape: (63985, 30)\n",
            "Columns: ['Programme', 'Project Number', 'CORDIS Link', 'Project Start Year', 'Project End Date', 'Project End Year', 'Project Title', 'Project Acronym', 'Project Status', 'Call ID', 'Call Deadline Date', 'Call Deadline Year', 'Project Signature Date', 'Project Signature Year', 'Project Start Date', 'Thematic Priority Descr', 'Pillar Abbr', 'Pillar Descr', 'Topic Code', 'Topic Descr', 'Simplified ToA', 'Legal Name', 'General PIC', 'Partner Role', 'Partner Type', 'Legal Entity Type', 'Signed Grants', 'Participation', 'EU Contribution', 'Total Cost']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
        "\n",
        "# 1. Load the CSV file with semicolon separator\n",
        "df = pd.read_csv('/content/processed_horizon_data.csv', sep=';')\n",
        "print(\"Original DataFrame shape:\", df.shape)\n",
        "print(\"\\nColumns in the dataset:\")\n",
        "print(df.columns.tolist())\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "def preprocess_horizon_data(df):\n",
        "    processed_data = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        qa_pairs = [\n",
        "            # Project Overview\n",
        "            {\n",
        "                'question': 'What are the basic details of this Horizon project?',\n",
        "                'answer': (f\"Programme: {row['Programme']}\\n\"\n",
        "                          f\"Project Number: {row['Project Number']}\\n\"\n",
        "                          f\"Project Title: {row['Project Title']}\\n\"\n",
        "                          f\"Project Acronym: {row['Project Acronym']}\"),\n",
        "                'category': 'project_overview'\n",
        "            },\n",
        "\n",
        "            # Timeline Information\n",
        "            {\n",
        "                'question': 'What are the key dates and status of this project?',\n",
        "                'answer': (f\"Start Date: {row['Project Start Date']}\\n\"\n",
        "                          f\"End Date: {row['Project End Date']}\\n\"\n",
        "                          f\"Status: {row['Project Status']}\\n\"\n",
        "                          f\"Call Deadline: {row['Call Deadline Date']}\"),\n",
        "                'category': 'timeline'\n",
        "            },\n",
        "\n",
        "            # Financial Information\n",
        "            {\n",
        "                'question': 'What is the financial information for this project?',\n",
        "                'answer': (f\"EU Contribution: {row['EU Contribution']}\\n\"\n",
        "                          f\"Total Cost: {row['Total Cost']}\\n\"\n",
        "                          f\"Participation: {row['Participation']}\"),\n",
        "                'category': 'financial'\n",
        "            },\n",
        "\n",
        "            # Topic and Programme Information\n",
        "            {\n",
        "                'question': 'What are the topic and programme details?',\n",
        "                'answer': (f\"Topic Code: {row['Topic Code']}\\n\"\n",
        "                          f\"Topic Description: {row['Topic Descr']}\\n\"\n",
        "                          f\"Pillar: {row['Pillar Descr']}\\n\"\n",
        "                          f\"Thematic Priority: {row['Thematic Priority Descr']}\"),\n",
        "                'category': 'topic'\n",
        "            },\n",
        "\n",
        "            # Partner Information\n",
        "            {\n",
        "                'question': 'What are the partner details?',\n",
        "                'answer': (f\"Legal Name: {row['Legal Name']}\\n\"\n",
        "                          f\"Partner Role: {row['Partner Role']}\\n\"\n",
        "                          f\"Partner Type: {row['Partner Type']}\\n\"\n",
        "                          f\"Legal Entity Type: {row['Legal Entity Type']}\"),\n",
        "                'category': 'partner'\n",
        "            }\n",
        "        ]\n",
        "        processed_data.extend(qa_pairs)\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "def clean_text(text):\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return \"Not specified\"\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s.,!?€$()-]', '', text)\n",
        "    return text\n",
        "\n",
        "# Clean the DataFrame\n",
        "print(\"Cleaning data...\")\n",
        "for column in df.columns:\n",
        "    if df[column].dtype == 'object':\n",
        "        df[column] = df[column].apply(clean_text)\n",
        "\n",
        "# Process the data\n",
        "print(\"Processing data...\")\n",
        "processed_data = preprocess_horizon_data(df)\n",
        "print(f\"Processed {len(processed_data)} QA pairs\")\n",
        "\n",
        "# Split into train and validation sets\n",
        "train_data, eval_data = train_test_split(processed_data, test_size=0.15, random_state=42)\n",
        "print(f\"Training examples: {len(train_data)}\")\n",
        "print(f\"Evaluation examples: {len(eval_data)}\")\n",
        "\n",
        "# Convert to Hugging Face datasets format\n",
        "def convert_to_hf_dataset(data_list):\n",
        "    return Dataset.from_dict({\n",
        "        'question': [item['question'] for item in data_list],\n",
        "        'answer': [item['answer'] for item in data_list],\n",
        "        'category': [item['category'] for item in data_list]\n",
        "    })\n",
        "\n",
        "# Create the datasets\n",
        "print(\"Creating datasets...\")\n",
        "train_dataset = convert_to_hf_dataset(train_data)\n",
        "eval_dataset = convert_to_hf_dataset(eval_data)\n",
        "\n",
        "# Print sample from dataset to verify\n",
        "print(\"\\nSample from training dataset:\")\n",
        "print(train_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy8lGXhZnmqM",
        "outputId": "be8fe17a-833c-4e0c-d14c-4c41c8d99a31"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame shape: (63985, 30)\n",
            "\n",
            "Columns in the dataset:\n",
            "['Programme', 'Project Number', 'CORDIS Link', 'Project Start Year', 'Project End Date', 'Project End Year', 'Project Title', 'Project Acronym', 'Project Status', 'Call ID', 'Call Deadline Date', 'Call Deadline Year', 'Project Signature Date', 'Project Signature Year', 'Project Start Date', 'Thematic Priority Descr', 'Pillar Abbr', 'Pillar Descr', 'Topic Code', 'Topic Descr', 'Simplified ToA', 'Legal Name', 'General PIC', 'Partner Role', 'Partner Type', 'Legal Entity Type', 'Signed Grants', 'Participation', 'EU Contribution', 'Total Cost']\n",
            "\n",
            "First few rows:\n",
            "                                           Programme  Project Number  \\\n",
            "0  H2020;115797;http://cordis.europa.eu/project/i...             NaN   \n",
            "1  H2020;115797;http://cordis.europa.eu/project/i...             NaN   \n",
            "2  H2020;115797;http://cordis.europa.eu/project/i...             NaN   \n",
            "3  H2020;115797;http://cordis.europa.eu/project/i...             NaN   \n",
            "4  H2020;115797;http://cordis.europa.eu/project/i...             NaN   \n",
            "\n",
            "  CORDIS Link Project Start Year Project End Date Project End Year  \\\n",
            "0         NaN                NaN              NaN              NaN   \n",
            "1         NaN                NaN              NaN              NaN   \n",
            "2         NaN                NaN              NaN              NaN   \n",
            "3         NaN                NaN              NaN              NaN   \n",
            "4         NaN                NaN              NaN              NaN   \n",
            "\n",
            "  Project Title Project Acronym Project Status Call ID  ... Simplified ToA  \\\n",
            "0           NaN             NaN            NaN     NaN  ...            NaN   \n",
            "1           NaN             NaN            NaN     NaN  ...            NaN   \n",
            "2           NaN             NaN            NaN     NaN  ...            NaN   \n",
            "3           NaN             NaN            NaN     NaN  ...            NaN   \n",
            "4           NaN             NaN            NaN     NaN  ...            NaN   \n",
            "\n",
            "   Legal Name General PIC  Partner Role Partner Type Legal Entity Type  \\\n",
            "0         NaN         NaN           NaN          NaN               NaN   \n",
            "1         NaN         NaN           NaN          NaN               NaN   \n",
            "2         NaN         NaN           NaN          NaN               NaN   \n",
            "3         NaN         NaN           NaN          NaN               NaN   \n",
            "4         NaN         NaN           NaN          NaN               NaN   \n",
            "\n",
            "  Signed Grants Participation EU Contribution Total Cost  \n",
            "0           NaN           NaN             NaN        NaN  \n",
            "1           NaN           NaN             NaN        NaN  \n",
            "2           NaN           NaN             NaN        NaN  \n",
            "3           NaN           NaN             NaN        NaN  \n",
            "4           NaN           NaN             NaN        NaN  \n",
            "\n",
            "[5 rows x 30 columns]\n",
            "Cleaning data...\n",
            "Processing data...\n",
            "Processed 319925 QA pairs\n",
            "Training examples: 271936\n",
            "Evaluation examples: 47989\n",
            "Creating datasets...\n",
            "\n",
            "Sample from training dataset:\n",
            "{'question': 'What are the topic and programme details?', 'answer': 'Topic Code: Not specified\\nTopic Description: Not specified\\nPillar: Not specified\\nThematic Priority: Not specified', 'category': 'topic'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's examine the data structure\n",
        "print(\"Sample of raw data:\")\n",
        "print(df.iloc[0])\n",
        "print(\"\\nColumn value counts for a sample column:\")\n",
        "print(df['Programme'].value_counts().head())\n",
        "\n",
        "# Modified preprocessing function with better error handling and data validation\n",
        "def preprocess_horizon_data(df):\n",
        "    processed_data = []\n",
        "    valid_entries = 0\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        # Only create QA pairs if essential fields are present\n",
        "        if pd.notna(row['Programme']) and pd.notna(row['Project Title']):\n",
        "            qa_pairs = [\n",
        "                # Project Overview\n",
        "                {\n",
        "                    'question': 'What are the basic details of this Horizon project?',\n",
        "                    'answer': (f\"Programme: {row['Programme']}\\n\"\n",
        "                              f\"Project Title: {row['Project Title']}\\n\"\n",
        "                              f\"Project Number: {row['Project Number']}\\n\"\n",
        "                              f\"Project Acronym: {row['Project Acronym']}\").strip(),\n",
        "                    'category': 'project_overview'\n",
        "                }\n",
        "            ]\n",
        "\n",
        "            # Add timeline information if dates are available\n",
        "            if pd.notna(row['Project Start Date']) or pd.notna(row['Project End Date']):\n",
        "                qa_pairs.append({\n",
        "                    'question': 'What are the key dates and status of this project?',\n",
        "                    'answer': (f\"Start Date: {row['Project Start Date']}\\n\"\n",
        "                              f\"End Date: {row['Project End Date']}\\n\"\n",
        "                              f\"Status: {row['Project Status']}\").strip(),\n",
        "                    'category': 'timeline'\n",
        "                })\n",
        "\n",
        "            # Add financial information if available\n",
        "            if pd.notna(row['EU Contribution']) or pd.notna(row['Total Cost']):\n",
        "                qa_pairs.append({\n",
        "                    'question': 'What is the financial information for this project?',\n",
        "                    'answer': (f\"EU Contribution: {row['EU Contribution']}\\n\"\n",
        "                              f\"Total Cost: {row['Total Cost']}\").strip(),\n",
        "                    'category': 'financial'\n",
        "                })\n",
        "\n",
        "            # Add topic information if available\n",
        "            if pd.notna(row['Topic Code']) or pd.notna(row['Topic Descr']):\n",
        "                qa_pairs.append({\n",
        "                    'question': 'What are the topic and programme details?',\n",
        "                    'answer': (f\"Topic Code: {row['Topic Code']}\\n\"\n",
        "                              f\"Topic Description: {row['Topic Descr']}\\n\"\n",
        "                              f\"Pillar: {row['Pillar Descr']}\").strip(),\n",
        "                    'category': 'topic'\n",
        "                })\n",
        "\n",
        "            # Add partner information if available\n",
        "            if pd.notna(row['Legal Name']) or pd.notna(row['Partner Role']):\n",
        "                qa_pairs.append({\n",
        "                    'question': 'What are the partner details?',\n",
        "                    'answer': (f\"Legal Name: {row['Legal Name']}\\n\"\n",
        "                              f\"Partner Role: {row['Partner Role']}\\n\"\n",
        "                              f\"Partner Type: {row['Partner Type']}\").strip(),\n",
        "                    'category': 'partner'\n",
        "                })\n",
        "\n",
        "            processed_data.extend(qa_pairs)\n",
        "            valid_entries += 1\n",
        "\n",
        "        # Print progress every 1000 rows\n",
        "        if idx % 1000 == 0:\n",
        "            print(f\"Processed {idx} rows, found {valid_entries} valid entries...\")\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and normalize text while preserving meaningful content.\"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # Keep more special characters that might be meaningful\n",
        "    text = re.sub(r'[^\\w\\s.,!?€$()-/]', '', text)\n",
        "\n",
        "    return text if text else \"\"\n",
        "\n",
        "# Clean the DataFrame\n",
        "print(\"Cleaning data...\")\n",
        "for column in df.columns:\n",
        "    if df[column].dtype == 'object':\n",
        "        df[column] = df[column].apply(clean_text)\n",
        "\n",
        "# Process the data\n",
        "print(\"Processing data...\")\n",
        "processed_data = preprocess_horizon_data(df)\n",
        "print(f\"\\nProcessed {len(processed_data)} total QA pairs\")\n",
        "\n",
        "# Print some statistics about the processed data\n",
        "categories = [item['category'] for item in processed_data]\n",
        "print(\"\\nDistribution of QA pairs by category:\")\n",
        "for category in set(categories):\n",
        "    count = categories.count(category)\n",
        "    print(f\"{category}: {count} pairs ({count/len(categories)*100:.2f}%)\")\n",
        "\n",
        "# Print some sample QA pairs\n",
        "print(\"\\nSample QA pairs:\")\n",
        "samples = []\n",
        "seen_categories = set()\n",
        "for item in processed_data:\n",
        "    if item['category'] not in seen_categories and item['answer'].strip() != \"\":\n",
        "        samples.append(item)\n",
        "        seen_categories.add(item['category'])\n",
        "        if len(seen_categories) == len(set(categories)):\n",
        "            break\n",
        "\n",
        "for sample in samples:\n",
        "    print(f\"\\nCategory: {sample['category']}\")\n",
        "    print(f\"Question: {sample['question']}\")\n",
        "    print(f\"Answer: {sample['answer']}\")\n",
        "\n",
        "# Split into train and validation sets\n",
        "train_data, eval_data = train_test_split(processed_data, test_size=0.15, random_state=42)\n",
        "print(f\"\\nTraining examples: {len(train_data)}\")\n",
        "print(f\"Evaluation examples: {len(eval_data)}\")\n",
        "\n",
        "# Convert to Hugging Face datasets format\n",
        "train_dataset = Dataset.from_dict({\n",
        "    'question': [item['question'] for item in train_data],\n",
        "    'answer': [item['answer'] for item in train_data],\n",
        "    'category': [item['category'] for item in train_data]\n",
        "})\n",
        "\n",
        "eval_dataset = Dataset.from_dict({\n",
        "    'question': [item['question'] for item in eval_data],\n",
        "    'answer': [item['answer'] for item in eval_data],\n",
        "    'category': [item['category'] for item in eval_data]\n",
        "})\n",
        "\n",
        "# Verify the quality of the datasets\n",
        "print(\"\\nVerifying dataset quality...\")\n",
        "print(\"\\nTraining dataset sample:\")\n",
        "for i in range(3):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"Question: {train_dataset[i]['question']}\")\n",
        "    print(f\"Answer: {train_dataset[i]['answer']}\")\n",
        "    print(f\"Category: {train_dataset[i]['category']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_naOUS5WoNWf",
        "outputId": "8197945b-49f8-420f-dac1-546c38a4df38"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of raw data:\n",
            "Programme                  H2020115797 approaches to disease modifying th...\n",
            "Project Number                                                           NaN\n",
            "CORDIS Link                                                    Not specified\n",
            "Project Start Year                                             Not specified\n",
            "Project End Date                                               Not specified\n",
            "Project End Year                                               Not specified\n",
            "Project Title                                                  Not specified\n",
            "Project Acronym                                                Not specified\n",
            "Project Status                                                 Not specified\n",
            "Call ID                                                        Not specified\n",
            "Call Deadline Date                                             Not specified\n",
            "Call Deadline Year                                                       NaN\n",
            "Project Signature Date                                         Not specified\n",
            "Project Signature Year                                                   NaN\n",
            "Project Start Date                                             Not specified\n",
            "Thematic Priority Descr                                        Not specified\n",
            "Pillar Abbr                                                    Not specified\n",
            "Pillar Descr                                                   Not specified\n",
            "Topic Code                                                     Not specified\n",
            "Topic Descr                                                    Not specified\n",
            "Simplified ToA                                                 Not specified\n",
            "Legal Name                                                     Not specified\n",
            "General PIC                                                              NaN\n",
            "Partner Role                                                   Not specified\n",
            "Partner Type                                                   Not specified\n",
            "Legal Entity Type                                              Not specified\n",
            "Signed Grants                                                            NaN\n",
            "Participation                                                            NaN\n",
            "EU Contribution                                                Not specified\n",
            "Total Cost                                                     Not specified\n",
            "Name: 0, dtype: object\n",
            "\n",
            "Column value counts for a sample column:\n",
            "Programme\n",
            "H2020                                                                                                                                                                                         28938\n",
            "Not specified                                                                                                                                                                                   157\n",
            "H2020739530 PPP - Aerosols, Clouds and Trace gases Preparatory Phase Project                                                                                                                     40\n",
            "H2020730900 and Earthquake Engineering Research Infrastructure Alliance for Europe                                                                                                               38\n",
            "H2020731944 and integrative analysis of regional, national and international Cohorts on primary Sjgrens Syndrome (pSS) towards improved stratification, treatment and health policy making       38\n",
            "Name: count, dtype: int64\n",
            "Cleaning data...\n",
            "Processing data...\n",
            "Processed 0 rows, found 1 valid entries...\n",
            "Processed 1000 rows, found 1001 valid entries...\n",
            "Processed 2000 rows, found 2001 valid entries...\n",
            "Processed 3000 rows, found 3001 valid entries...\n",
            "Processed 4000 rows, found 4001 valid entries...\n",
            "Processed 5000 rows, found 5001 valid entries...\n",
            "Processed 6000 rows, found 6001 valid entries...\n",
            "Processed 7000 rows, found 7001 valid entries...\n",
            "Processed 8000 rows, found 8001 valid entries...\n",
            "Processed 9000 rows, found 9001 valid entries...\n",
            "Processed 10000 rows, found 10001 valid entries...\n",
            "Processed 11000 rows, found 11001 valid entries...\n",
            "Processed 12000 rows, found 12001 valid entries...\n",
            "Processed 13000 rows, found 13001 valid entries...\n",
            "Processed 14000 rows, found 14001 valid entries...\n",
            "Processed 15000 rows, found 15001 valid entries...\n",
            "Processed 16000 rows, found 16001 valid entries...\n",
            "Processed 17000 rows, found 17001 valid entries...\n",
            "Processed 18000 rows, found 18001 valid entries...\n",
            "Processed 19000 rows, found 19001 valid entries...\n",
            "Processed 20000 rows, found 20001 valid entries...\n",
            "Processed 21000 rows, found 21001 valid entries...\n",
            "Processed 22000 rows, found 22001 valid entries...\n",
            "Processed 23000 rows, found 23001 valid entries...\n",
            "Processed 24000 rows, found 24001 valid entries...\n",
            "Processed 25000 rows, found 25001 valid entries...\n",
            "Processed 26000 rows, found 26001 valid entries...\n",
            "Processed 27000 rows, found 27001 valid entries...\n",
            "Processed 28000 rows, found 28001 valid entries...\n",
            "Processed 29000 rows, found 29001 valid entries...\n",
            "Processed 30000 rows, found 30001 valid entries...\n",
            "Processed 31000 rows, found 31001 valid entries...\n",
            "Processed 32000 rows, found 32001 valid entries...\n",
            "Processed 33000 rows, found 33001 valid entries...\n",
            "Processed 34000 rows, found 34001 valid entries...\n",
            "Processed 35000 rows, found 35001 valid entries...\n",
            "Processed 36000 rows, found 36001 valid entries...\n",
            "Processed 37000 rows, found 37001 valid entries...\n",
            "Processed 38000 rows, found 38001 valid entries...\n",
            "Processed 39000 rows, found 39001 valid entries...\n",
            "Processed 40000 rows, found 40001 valid entries...\n",
            "Processed 41000 rows, found 41001 valid entries...\n",
            "Processed 42000 rows, found 42001 valid entries...\n",
            "Processed 43000 rows, found 43001 valid entries...\n",
            "Processed 44000 rows, found 44001 valid entries...\n",
            "Processed 45000 rows, found 45001 valid entries...\n",
            "Processed 46000 rows, found 46001 valid entries...\n",
            "Processed 47000 rows, found 47001 valid entries...\n",
            "Processed 48000 rows, found 48001 valid entries...\n",
            "Processed 49000 rows, found 49001 valid entries...\n",
            "Processed 50000 rows, found 50001 valid entries...\n",
            "Processed 51000 rows, found 51001 valid entries...\n",
            "Processed 52000 rows, found 52001 valid entries...\n",
            "Processed 53000 rows, found 53001 valid entries...\n",
            "Processed 54000 rows, found 54001 valid entries...\n",
            "Processed 55000 rows, found 55001 valid entries...\n",
            "Processed 56000 rows, found 56001 valid entries...\n",
            "Processed 57000 rows, found 57001 valid entries...\n",
            "Processed 58000 rows, found 58001 valid entries...\n",
            "Processed 59000 rows, found 59001 valid entries...\n",
            "Processed 60000 rows, found 60001 valid entries...\n",
            "Processed 61000 rows, found 61001 valid entries...\n",
            "Processed 62000 rows, found 62001 valid entries...\n",
            "Processed 63000 rows, found 63001 valid entries...\n",
            "\n",
            "Processed 319925 total QA pairs\n",
            "\n",
            "Distribution of QA pairs by category:\n",
            "topic: 63985 pairs (20.00%)\n",
            "partner: 63985 pairs (20.00%)\n",
            "project_overview: 63985 pairs (20.00%)\n",
            "financial: 63985 pairs (20.00%)\n",
            "timeline: 63985 pairs (20.00%)\n",
            "\n",
            "Sample QA pairs:\n",
            "\n",
            "Category: project_overview\n",
            "Question: What are the basic details of this Horizon project?\n",
            "Answer: Programme: H2020115797 approaches to disease modifying therapy of type 1 diabetes an innovative approach towards understanding and arresting type 1 diabetes Sofia ref. 115797INNODIACLOSEDH2020-JTI-IMI2-2014-01-two-stage14.04.201520157.12.201520151.11.2015Health, demographic change and wellbeingEU.3.Societal ChallengesIMI2-2014-01-01Translational approaches to disease modifying therapy of Type 1 Diabetes Mellitus (T1DM)RIAACADEMISCH ZIEKENHUIS LEIDEN999990849PARTICIPANTBENEFICIARYHES11542.2451.059.099\n",
            "Project Title: Not specified\n",
            "Project Number: nan\n",
            "Project Acronym: Not specified\n",
            "\n",
            "Category: timeline\n",
            "Question: What are the key dates and status of this project?\n",
            "Answer: Start Date: Not specified\n",
            "End Date: Not specified\n",
            "Status: Not specified\n",
            "\n",
            "Category: financial\n",
            "Question: What is the financial information for this project?\n",
            "Answer: EU Contribution: Not specified\n",
            "Total Cost: Not specified\n",
            "\n",
            "Category: topic\n",
            "Question: What are the topic and programme details?\n",
            "Answer: Topic Code: Not specified\n",
            "Topic Description: Not specified\n",
            "Pillar: Not specified\n",
            "\n",
            "Category: partner\n",
            "Question: What are the partner details?\n",
            "Answer: Legal Name: Not specified\n",
            "Partner Role: Not specified\n",
            "Partner Type: Not specified\n",
            "\n",
            "Training examples: 271936\n",
            "Evaluation examples: 47989\n",
            "\n",
            "Verifying dataset quality...\n",
            "\n",
            "Training dataset sample:\n",
            "\n",
            "Example 1:\n",
            "Question: What are the topic and programme details?\n",
            "Answer: Topic Code: Not specified\n",
            "Topic Description: Not specified\n",
            "Pillar: Not specified\n",
            "Category: topic\n",
            "\n",
            "Example 2:\n",
            "Question: What are the key dates and status of this project?\n",
            "Answer: Start Date: Not specified\n",
            "End Date: Not specified\n",
            "Status: Not specified\n",
            "Category: timeline\n",
            "\n",
            "Example 3:\n",
            "Question: What are the partner details?\n",
            "Answer: Legal Name: CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE CNRS\n",
            "Partner Role: PARTICIPANT\n",
            "Partner Type: BENEFICIARY\n",
            "Category: partner\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('/content/processed_horizon_data.csv', sep=';')\n",
        "print(\"Original data shape:\", df.shape)\n",
        "\n",
        "def create_qa_pairs(df):\n",
        "    \"\"\"\n",
        "    Create question-answer pairs from the Horizon dataset.\n",
        "    \"\"\"\n",
        "    qa_pairs = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        # Create multiple QA pairs for each project\n",
        "        pairs = [\n",
        "            {\n",
        "                'question': 'What is the basic information about this project?',\n",
        "                'answer': f\"Project Title: {row['Project Title']}\\nProgramme: {row['Programme']}\\nProject Number: {row['Project Number']}\\nAcronym: {row['Project Acronym']}\",\n",
        "                'category': 'basic_info'\n",
        "            },\n",
        "            {\n",
        "                'question': 'What are the project dates and timeline?',\n",
        "                'answer': f\"Start Date: {row['Project Start Date']}\\nEnd Date: {row['Project End Date']}\\nStatus: {row['Project Status']}\",\n",
        "                'category': 'timeline'\n",
        "            },\n",
        "            {\n",
        "                'question': 'What is the financial information?',\n",
        "                'answer': f\"EU Contribution: {row['EU Contribution']}\\nTotal Cost: {row['Total Cost']}\",\n",
        "                'category': 'financial'\n",
        "            },\n",
        "            {\n",
        "                'question': 'What are the project topics and priorities?',\n",
        "                'answer': f\"Topic: {row['Topic Descr']}\\nPillar: {row['Pillar Descr']}\\nThematic Priority: {row['Thematic Priority Descr']}\",\n",
        "                'category': 'topics'\n",
        "            }\n",
        "        ]\n",
        "        qa_pairs.extend(pairs)\n",
        "\n",
        "    return qa_pairs\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and normalize text.\"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return \"Not available\"\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # Remove special characters but keep basic punctuation\n",
        "    text = re.sub(r'[^\\w\\s.,!?€$()-]', '', text)\n",
        "    return text\n",
        "\n",
        "# Clean the DataFrame\n",
        "print(\"Cleaning data...\")\n",
        "for column in df.columns:\n",
        "    if df[column].dtype == 'object':\n",
        "        df[column] = df[column].apply(clean_text)\n",
        "\n",
        "# Create QA pairs\n",
        "print(\"Creating QA pairs...\")\n",
        "qa_pairs = create_qa_pairs(df)\n",
        "print(f\"Created {len(qa_pairs)} QA pairs\")\n",
        "\n",
        "# Split into train and validation sets\n",
        "train_data, eval_data = train_test_split(\n",
        "    qa_pairs, test_size=0.15, random_state=42\n",
        ")\n",
        "print(f\"Training set size: {len(train_data)}\")\n",
        "print(f\"Validation set size: {len(eval_data)}\")\n",
        "\n",
        "# Convert to Hugging Face datasets format\n",
        "def convert_to_hf_dataset(data_list):\n",
        "    return Dataset.from_dict({\n",
        "        'question': [item['question'] for item in data_list],\n",
        "        'answer': [item['answer'] for item in data_list],\n",
        "        'category': [item['category'] for item in data_list]\n",
        "    })\n",
        "\n",
        "# Create the datasets\n",
        "print(\"Creating Hugging Face datasets...\")\n",
        "train_dataset = convert_to_hf_dataset(train_data)\n",
        "eval_dataset = convert_to_hf_dataset(eval_data)\n",
        "\n",
        "# Print some examples to verify the data\n",
        "print(\"\\nExample QA pairs from training dataset:\")\n",
        "for i in range(3):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"Question: {train_dataset[i]['question']}\")\n",
        "    print(f\"Answer: {train_dataset[i]['answer']}\")\n",
        "    print(f\"Category: {train_dataset[i]['category']}\")\n",
        "\n",
        "# Print dataset statistics\n",
        "print(\"\\nDataset Statistics:\")\n",
        "categories = [item['category'] for item in qa_pairs]\n",
        "for category in set(categories):\n",
        "    count = categories.count(category)\n",
        "    percentage = (count / len(categories)) * 100\n",
        "    print(f\"{category}: {count} pairs ({percentage:.2f}%)\")\n",
        "\n",
        "# Save datasets (optional)\n",
        "train_dataset.save_to_disk('horizon_train_dataset')\n",
        "eval_dataset.save_to_disk('horizon_eval_dataset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726,
          "referenced_widgets": [
            "57aec71d984e4afd97791536503dc848",
            "fa3ea53576204ab9a2e10cba806c79ff",
            "8509ec49d1bd4e0db7845d43aec7047d",
            "10a704225a1a4ca982d9f9d82d8e69ac",
            "fd4533cc2f3440918751a707175b1b9d",
            "ea9ad09944b34b3ea265ee46fd292eee",
            "826ce4caab414666a94a0d60e5237a0d",
            "8023fd55dab849f0bd4caff004f9b956",
            "a8fea6dc3ca74ac9b5155bf8cc7de463",
            "314211d5d97747b0aa73462e2bd25986",
            "165c13d92e0c4c1085659f6fee351bab",
            "2e5d13ae1fa74dd0b967db0e2d966461",
            "3a6f1c601cc3494e9d1aeca0f3798a90",
            "252f320ed1b44aa0b696bea5e82f5ff8",
            "afbd6e01152344b6ac1f302a2d0230f3",
            "a8394f8d6a7b4ac2907946d69dba10f8",
            "4b9ee30749604074970e57b32d833921",
            "f26748a225ea4ad2946bb09230c81054",
            "335514403de14e779f772a1218adbccc",
            "0d19d8200b834973a3fb00113a42845f",
            "e48040f5ad924915ab9cdf4fef5adcc9",
            "2f33511912694c4cb944972272b08287"
          ]
        },
        "id": "ujXZwK6mo8_c",
        "outputId": "1f4b088b-d45c-4176-b503-47f01420bcb9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data shape: (63985, 30)\n",
            "Cleaning data...\n",
            "Creating QA pairs...\n",
            "Created 255940 QA pairs\n",
            "Training set size: 217549\n",
            "Validation set size: 38391\n",
            "Creating Hugging Face datasets...\n",
            "\n",
            "Example QA pairs from training dataset:\n",
            "\n",
            "Example 1:\n",
            "Question: What is the financial information?\n",
            "Answer: EU Contribution: 165.506\n",
            "Total Cost: 236.438\n",
            "Category: financial\n",
            "\n",
            "Example 2:\n",
            "Question: What are the project dates and timeline?\n",
            "Answer: Start Date: Not available\n",
            "End Date: Not available\n",
            "Status: Not available\n",
            "Category: timeline\n",
            "\n",
            "Example 3:\n",
            "Question: What is the basic information about this project?\n",
            "Answer: Project Title: Not available\n",
            "Programme: H2020689682 Management of Barriers in European RiversAMBERCLOSEDH2020-SC5-2015-two-stage8.09.2015201529.04.201620161.06.2016Climate action, environment, resource efficiency and raw materialsEU.3.Societal ChallengesSC5-07-2015More effective ecosystem restoration in the EURIAUNIVERSIDAD DE OVIEDO999848647PARTICIPANTBENEFICIARYHES11209.129209.129\n",
            "Project Number: nan\n",
            "Acronym: Not available\n",
            "Category: basic_info\n",
            "\n",
            "Dataset Statistics:\n",
            "topics: 63985 pairs (25.00%)\n",
            "financial: 63985 pairs (25.00%)\n",
            "basic_info: 63985 pairs (25.00%)\n",
            "timeline: 63985 pairs (25.00%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/217549 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57aec71d984e4afd97791536503dc848"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/38391 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e5d13ae1fa74dd0b967db0e2d966461"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UEkhcigVpOrS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN//hUy/LzxrnwbdLRA0Kdo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "57aec71d984e4afd97791536503dc848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa3ea53576204ab9a2e10cba806c79ff",
              "IPY_MODEL_8509ec49d1bd4e0db7845d43aec7047d",
              "IPY_MODEL_10a704225a1a4ca982d9f9d82d8e69ac"
            ],
            "layout": "IPY_MODEL_fd4533cc2f3440918751a707175b1b9d"
          }
        },
        "fa3ea53576204ab9a2e10cba806c79ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea9ad09944b34b3ea265ee46fd292eee",
            "placeholder": "​",
            "style": "IPY_MODEL_826ce4caab414666a94a0d60e5237a0d",
            "value": "Saving the dataset (1/1 shards): 100%"
          }
        },
        "8509ec49d1bd4e0db7845d43aec7047d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8023fd55dab849f0bd4caff004f9b956",
            "max": 217549,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a8fea6dc3ca74ac9b5155bf8cc7de463",
            "value": 217549
          }
        },
        "10a704225a1a4ca982d9f9d82d8e69ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_314211d5d97747b0aa73462e2bd25986",
            "placeholder": "​",
            "style": "IPY_MODEL_165c13d92e0c4c1085659f6fee351bab",
            "value": " 217549/217549 [00:00&lt;00:00, 934675.20 examples/s]"
          }
        },
        "fd4533cc2f3440918751a707175b1b9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea9ad09944b34b3ea265ee46fd292eee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "826ce4caab414666a94a0d60e5237a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8023fd55dab849f0bd4caff004f9b956": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8fea6dc3ca74ac9b5155bf8cc7de463": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "314211d5d97747b0aa73462e2bd25986": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "165c13d92e0c4c1085659f6fee351bab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e5d13ae1fa74dd0b967db0e2d966461": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a6f1c601cc3494e9d1aeca0f3798a90",
              "IPY_MODEL_252f320ed1b44aa0b696bea5e82f5ff8",
              "IPY_MODEL_afbd6e01152344b6ac1f302a2d0230f3"
            ],
            "layout": "IPY_MODEL_a8394f8d6a7b4ac2907946d69dba10f8"
          }
        },
        "3a6f1c601cc3494e9d1aeca0f3798a90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b9ee30749604074970e57b32d833921",
            "placeholder": "​",
            "style": "IPY_MODEL_f26748a225ea4ad2946bb09230c81054",
            "value": "Saving the dataset (1/1 shards): 100%"
          }
        },
        "252f320ed1b44aa0b696bea5e82f5ff8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_335514403de14e779f772a1218adbccc",
            "max": 38391,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d19d8200b834973a3fb00113a42845f",
            "value": 38391
          }
        },
        "afbd6e01152344b6ac1f302a2d0230f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e48040f5ad924915ab9cdf4fef5adcc9",
            "placeholder": "​",
            "style": "IPY_MODEL_2f33511912694c4cb944972272b08287",
            "value": " 38391/38391 [00:00&lt;00:00, 697999.18 examples/s]"
          }
        },
        "a8394f8d6a7b4ac2907946d69dba10f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b9ee30749604074970e57b32d833921": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f26748a225ea4ad2946bb09230c81054": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "335514403de14e779f772a1218adbccc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d19d8200b834973a3fb00113a42845f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e48040f5ad924915ab9cdf4fef5adcc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f33511912694c4cb944972272b08287": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}